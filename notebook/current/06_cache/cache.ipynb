{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: VECLIB_MAXIMUM_THREADS=1\n",
      "env: MKL_NUM_THREADS=1\n",
      "env: NUMEXPR_NUM_THREADS=1\n",
      "env: OMP_NUM_THREADS=1\n",
      "rm -rf *.o *.dSYM/ 01_skip_access 02_locality 03_matrix_matrix\r\n"
     ]
    }
   ],
   "source": [
    "%env VECLIB_MAXIMUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1\n",
    "\n",
    "!make clean\n",
    "\n",
    "import numpy as np\n",
    "#np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache optimization\n",
    "\n",
    "1. Memory hierarchy\n",
    "2. How cache works\n",
    "   1. Cache block (line) size determines speed\n",
    "   2. Locality\n",
    "   3. Matrix population in C++\n",
    "   4. Array majoring in numpy\n",
    "3. Tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory hierarchy\n",
    "\n",
    "In the simplest model of computers, we write programs that process data in a linearly-addressed memory.  In reality, to achieve good runtime performance, the memory is hierarchical.  The programs we write still treat it like a linearly-addressed space, but by understanding the underneath hierarchical structure, we may get the most performance from the system.\n",
    "\n",
    "When it comes to performance, we may be tempted to make all memory as fast as possible.  It is impractical because fast memory is very difficult to manufacture.  The economocally feasible approach is to keep the most frequently used data in the fastest memory, and the infrequently used data in slow and inexpensive memory.  When the work with the data is finished, they are removed from the fast memory, and other data are loaded to it.\n",
    "\n",
    "Memory comes in many kinds in the hierarchy.  I put them in 4 categories:\n",
    "\n",
    "1. CPU register file\n",
    "\n",
    "   The registers reside in the CPU circuits.  Instructions (machine code or assembly) can directly operate them and the electronic signals flow through the CPU circuits.  There is no delay in access time to registers.  They are the fastest memory.\n",
    "2. CPU cache\n",
    "\n",
    "   CPU cache memory works as a 'buffer' between the registers and the main memory.  It usually uses fast and expensive static random access memory (SRAM).  It is called static because the circuit keeps in one of the two stable states and access doesn't change the state.  The circuit takes more transistors than slower types of memory.\n",
    "\n",
    "   The cache memory may be part of the CPU circuit or outside it.  A CPU usually has multiple levels of cache.  It can be a couple of MBs, or as large as tens or hundreds of MBs:\n",
    "   * Intel Xeon Platinum 9282 has 77MB L3 (56 cores): https://en.wikichip.org/wiki/intel/xeon_platinum/9282\n",
    "   * AMD EPYC 7H12 has 256MB L3 (64 cores): https://www.amd.com/en/products/cpu/amd-epyc-7h12\n",
    "3. Main memory\n",
    "\n",
    "   Main memory is away from the CPU chip package.  It usually uses less expensive dynamic random access memory (DRAM).  The circuit takes more time to access the data but the lower cost allows much larger space.\n",
    "\n",
    "   Data in the main memory are lost when the system is powered off.\n",
    "\n",
    "   Mainstream PC uses DDR (double data rate) 4 SDRAM (synchronous dynamic random-access memory) with DIMM (dual in-line memory module) and its variants, e.g. RDIMM (registered DIMM) and LRDIMM (load reduced DIMM).  Depending on the CPU memory controller bandwidth, the data throughput may be around 60GB/s or higher.\n",
    "   \n",
    "   A powerful server may have up to 6TB of main memory:\n",
    "   * 24 RDIMM/LRDIMM slots: https://www.supermicro.com/en/products/ultra \n",
    "   * 256GB RDIMM/LRDIMM module: https://www.samsung.com/semiconductor/dram/module/\n",
    "4. Storage\n",
    "\n",
    "   Data on the storage cannot be directly accessed by CPU instructions.  They need to be loaded to main memory and then the instructions can touch them.  The loading and saving operations are considered input and output (I/O).  Data in the storage are presisted when the system is powered off.\n",
    "\n",
    "   The storage is usually called the \"disks\", because it used to be hard-disk drives (HDD).  In a recent system the storage changes to use the solid-state drives (SSD), which use the flash memory instead of hard disk.\n",
    "   * Seagate Burracuda 510 SSD sequential read 3.45 GB/s, write 3.2 GB/s: https://www.seagate.com/internal-hard-drives/ssd/barracuda-ssd/\n",
    "   * Samsung PM1735 SSD sequential read 8 GB/s, write 3.8 GB/s: https://www.samsung.com/semiconductor/ssd/enterprise-ssd/\n",
    "\n",
    "Here is a table (excerpt from Figure 6.23 in CS:APP (3/e)) showing the latency of each of the memory, measured in CPU cycle (less than 1 ns):\n",
    "\n",
    "| Type | Latency (cycles) |\n",
    "| -- | -- |\n",
    "| CPU registers | 0 |\n",
    "| L1 cache | 4 | \n",
    "| L2 cache | 10 |\n",
    "| L3 cache | 50 |\n",
    "| Main memory | 200 |\n",
    "| Storage (disk) | 100,000 |\n",
    "\n",
    "Nothing is fast without cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How cache works\n",
    "\n",
    "There are 3 ways to implement caches: (i) direct-map caches, (ii) set-associative caches, and (iii) fully-associative caches.  The direct-map caches are the simplest one and I will use it to show the basic concepts of caches.\n",
    "\n",
    "If the accessed byte is already in the cache, it's a hit, and CPU directly gets the byte from the cache.  If the byte is not in the cache, it's a miss, and the memory controller goes to the main memory to fetch the byte into cache, before CPU gets it.\n",
    "\n",
    "According to the previous table, a cache miss is expensive.  When CPU can get data from cache, the latency is around a couple of cycles.  When there is a miss, additional hundreds of cycles are required to get the data.\n",
    "\n",
    "There are two kinds of misses.  A cold miss happens when the requested byte is not in the cache.  The second kind is conflict miss, and happens when multiple cachable data claim the same cache block.  One set pops out the other, and vice versa, and each access is a miss.\n",
    "\n",
    "Assume we have a main memory of 64 bytes (6-bit address is sufficient), and a cache of 8 bytes (use 3 bits for addressing).  The following example demonstrates how a cache works:\n",
    "\n",
    "| Access # | Decimal addr | Binary addr | Hit or miss | Cache block |\n",
    "| -- | -- | -- | -- | -- |\n",
    "| 1 | 22 | 010 110 | miss (cold) | 110 |\n",
    "| 2 | 26 | 011 010 | miss (cold) | 010 |\n",
    "| 3 | 22 | 010 110 | hit | 110 |\n",
    "| 4 | 26 | 011 010 | hit | 010 |\n",
    "| 5 | 16 | 010 000 | miss (cold) | 000 |\n",
    "| 6 | 18 | 010 010 | miss (cold) | 010 |\n",
    "| 7 | 26 | 011 010 | miss (conflict) | 010 |\n",
    "| 8 | 18 | 010 010 | miss (conflict) | 010 |\n",
    "\n",
    "This is a direct-map cache.  To reduce conflict misses, we may use multi-way set associativity.  2-, 4-, or 8-way set associative cache is commonly used.  Full associativity is too expensive in circuit implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache block (line) size determines speed\n",
    "\n",
    "A cache block usually contains more than one byte or word.  In x86, the block (line) size is 64 bytes.  When loading data from main memory to cache, it's done block by block, rather than byte by byte.\n",
    "\n",
    "I will be using an example of \"skip access\" to demonstrate that with cache, doing more things doesn't take more time.\n",
    "\n",
    "Before the example, I am showing the C++11-based timer used in the experiments:\n",
    "\n",
    "```cpp\n",
    "#pragma once\n",
    "\n",
    "#include <chrono>\n",
    "\n",
    "class StopWatch\n",
    "{\n",
    "\n",
    "private:\n",
    "\n",
    "    using clock_type = std::chrono::high_resolution_clock;\n",
    "    using time_type = std::chrono::time_point<clock_type>;\n",
    "\n",
    "public:\n",
    "\n",
    "    /// A singleton.\n",
    "    static StopWatch & me()\n",
    "    {\n",
    "        static StopWatch instance;\n",
    "        return instance;\n",
    "    }\n",
    "\n",
    "    StopWatch() : m_start(clock_type::now()), m_stop(m_start) {}\n",
    "\n",
    "    StopWatch(StopWatch const & ) = default;\n",
    "    StopWatch(StopWatch       &&) = default;\n",
    "    StopWatch & operator=(StopWatch const & ) = default;\n",
    "    StopWatch & operator=(StopWatch       &&) = default;\n",
    "    ~StopWatch() = default;\n",
    "\n",
    "    /**\n",
    "     * Return seconds between laps.\n",
    "     */\n",
    "    double lap()\n",
    "    {\n",
    "        m_start = m_stop;\n",
    "        m_stop = clock_type::now();\n",
    "        return std::chrono::duration<double>(m_stop - m_start).count();\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Return seconds between end and start.\n",
    "     */\n",
    "    double duration() const { return std::chrono::duration<double>(m_stop - m_start).count(); }\n",
    "\n",
    "    /**\n",
    "     * Return resolution in second.\n",
    "     */\n",
    "    static constexpr double resolution()\n",
    "    {\n",
    "        return double(clock_type::period::num) / double(clock_type::period::den);\n",
    "    }\n",
    "\n",
    "private:\n",
    "\n",
    "    time_type m_start;\n",
    "    time_type m_stop;\n",
    "\n",
    "}; /* end struct StopWatch */\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark by skip access\n",
    "\n",
    "This is an example for skip access.  We allocate a memory buffer of `128 * 1024 * 1024` bytes (128 MB), and operate the contents with different \"skips\".  Skip 1 means accessing every element.  Skip 2 accesses half of the elements, and so on.\n",
    "\n",
    "```cpp\n",
    "constexpr const size_t nelem = 128 * 1024 * 1024;\n",
    "int * arr = new int[nelem];\n",
    "\n",
    "// Sequential.\n",
    "for (int i=0; i<nelem; ++i) { arr[i] = i; }\n",
    "sw.lap();\n",
    "for (int i=0; i<nelem; ++i) { arr[i] *= 3; }\n",
    "elapsed = sw.lap();\n",
    "\n",
    "// Skipping 2.\n",
    "for (int i=0; i<nelem; ++i) { arr[i] = i; }\n",
    "sw.lap();\n",
    "for (int i=0; i<nelem; i+=2) { arr[i] *= 3; }\n",
    "elapsed = sw.lap();\n",
    "\n",
    "// ... 4, 8, 16, ... 1024.\n",
    "```\n",
    "\n",
    "Without knowing the effect of cache, we might intuitively think that the more skip the shorter the runtime.  It's not wrong, but not completely correct.  In the experiment, we see that the skipping has an effect of runtime only after certain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 01_skip_access 01_skip_access.cpp\n",
      "Sequential takes: 0.0781284 sec\n",
      "\n",
      "Skipping 2 takes: 0.0785412 sec\n",
      "Skipping 4 takes: 0.0657564 sec\n",
      "Skipping 8 takes: 0.0676219 sec\n",
      "Skipping 16 takes: 0.0684591 sec\n",
      "\n",
      "Skipping 32 takes: 0.0466138 sec\n",
      "Skipping 64 takes: 0.03069 sec\n",
      "Skipping 128 takes: 0.0196085 sec\n",
      "Skipping 256 takes: 0.0114839 sec\n",
      "Skipping 512 takes: 0.00617526 sec\n",
      "Skipping 1024 takes: 0.00222318 sec\n"
     ]
    }
   ],
   "source": [
    "# Show how main memory (dram) access dominates runtime.\n",
    "!make 01_skip_access ; ./01_skip_access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality\n",
    "\n",
    "While coding we usually don't have a lot of time to do detailed cache analysis.  Instead, we keep in mind that the code runs faster when it's more compact.  This is the concept of locality.\n",
    "\n",
    "There are two kinds of locality: temporal and spatial.  Temporal locality means that a fixed address is reused in the near future.  Spatial locality means that the addresses close to the current address is reused in the near future.  The better locality, of either kind, improves the performance.  And the cache hierarchy is why locality works.\n",
    "\n",
    "To take advantage of locality, programmers analyze by using \"strides\".  A stride is the number of indexes to elements to slide when accessing the data in arrays.  The most basic striding is sequential access, or the 1-stride.  Similarly, we may have n-strides.  The larger the stride is, the worse the locality.\n",
    "\n",
    "Recall that x86 uses 64-byte cache blocks, and a double-precision floating point takes 8 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix population in C++\n",
    "\n",
    "To demonstrate the data layout, i.e., majoring or striding, affects runtime, we use an example of populating $1024 \\times 1024 \\times 64$ integer elements as a matrix.  The following shapes are benchmarked (total number of elements remains unchanged):\n",
    "\n",
    "* $(1024\\times1024\\times64) \\times 1$, i.e., one-dimension\n",
    "* $(1024\\times1024\\times32) \\times 2$\n",
    "* $(1024\\times1024\\times16) \\times 4$\n",
    "* $(1024\\times1024\\times8) \\times 8$\n",
    "* $(1024\\times1024\\times4) \\times 16$\n",
    "* $(1024\\times1024\\times2) \\times 32$\n",
    "* $(1024\\times1024) \\times 64$\n",
    "* $(1024\\times512) \\times 128$\n",
    "* $(1024\\times256) \\times 256$\n",
    "* $(1024\\times128) \\times 512$\n",
    "* $(1024\\times64) \\times 1024$\n",
    "* $(1024\\times32) \\times (1024\\times2)$\n",
    "* $(1024\\times16) \\times (1024\\times4)$\n",
    "* $(1024\\times8) \\times (1024\\times8)$\n",
    "\n",
    "We populate the matrices along two axes.  First we iterate over the last index (row):\n",
    "\n",
    "```cpp\n",
    "// Populate by last index.\n",
    "for (size_t i=0; i<nrow; ++i) // the i-th row\n",
    "{\n",
    "    for (size_t j=0; j<ncol; ++j) // the j-th column\n",
    "    {\n",
    "        buffer[i*ncol + j] = i*ncol + j;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Then iterate over the first index (column):\n",
    "\n",
    "```cpp\n",
    "// Populate by first index.\n",
    "for (size_t j=0; j<ncol; ++j) // the j-th column\n",
    "{\n",
    "    for (size_t i=0; i<nrow; ++i) // the i-th row\n",
    "    {\n",
    "        buffer[i*ncol + j] = i*ncol + j;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We will see the speed is very different.  To get the benchmark results correct, before the first benchmarked population, we should access everywhere in the buffer to make sure the memory is allocated:\n",
    "\n",
    "```cpp\n",
    "// Prepopulation to cancel the effect of overcommit or delayed allocation.\n",
    "for (size_t i=0; i<nelem; ++i) { buffer[i] = nelem-i; }\n",
    "```\n",
    "\n",
    "While writing programs, it's much easier to know the stride than analyzing the cache behavior.  The latter, in many scenarios, is prohibitively difficult.\n",
    "\n",
    "Since we know the cache line is 64 byte wide, we expect the cache performance may significantly reduce when the stride is around that value (16 int elements).  As shown in the above benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 02_locality 02_locality.cpp\n",
      "# of elements: 67108864 = 67108864 x 1\n",
      "populate double flatly takes: 0.0596488 sec\n",
      "populate double along last axis takes: 0.137351 sec\n",
      "populate double along first axis takes: 0.0765683 sec\n",
      "ratio: 0.557466\n",
      "\n",
      "# of elements: 67108864 = 33554432 x 2\n",
      "populate double flatly takes: 0.0629023 sec\n",
      "populate double along last axis takes: 0.0998501 sec\n",
      "populate double along first axis takes: 0.113742 sec\n",
      "ratio: 1.13913\n",
      "\n",
      "# of elements: 67108864 = 16777216 x 4\n",
      "populate double flatly takes: 0.0620802 sec\n",
      "populate double along last axis takes: 0.0628614 sec\n",
      "populate double along first axis takes: 0.230874 sec\n",
      "ratio: 3.67275\n",
      "\n",
      "# of elements: 67108864 = 8388608 x 8\n",
      "populate double flatly takes: 0.0534204 sec\n",
      "populate double along last axis takes: 0.0610474 sec\n",
      "populate double along first axis takes: 0.439286 sec\n",
      "ratio: 7.19581\n",
      "\n",
      "# of elements: 67108864 = 4194304 x 16\n",
      "populate double flatly takes: 0.066121 sec\n",
      "populate double along last axis takes: 0.0623013 sec\n",
      "populate double along first axis takes: 0.574188 sec\n",
      "ratio: 9.2163\n",
      "\n",
      "# of elements: 67108864 = 2097152 x 32\n",
      "populate double flatly takes: 0.0608135 sec\n",
      "populate double along last axis takes: 0.0625608 sec\n",
      "populate double along first axis takes: 0.76468 sec\n",
      "ratio: 12.223\n",
      "\n",
      "# of elements: 67108864 = 1048576 x 64\n",
      "populate double flatly takes: 0.0589263 sec\n",
      "populate double along last axis takes: 0.0653641 sec\n",
      "populate double along first axis takes: 0.861749 sec\n",
      "ratio: 13.1838\n",
      "\n",
      "# of elements: 67108864 = 524288 x 128\n",
      "populate double flatly takes: 0.0612724 sec\n",
      "populate double along last axis takes: 0.0604812 sec\n",
      "populate double along first axis takes: 0.900117 sec\n",
      "ratio: 14.8826\n",
      "\n",
      "# of elements: 67108864 = 262144 x 256\n",
      "populate double flatly takes: 0.0607006 sec\n",
      "populate double along last axis takes: 0.0537362 sec\n",
      "populate double along first axis takes: 0.833298 sec\n",
      "ratio: 15.5072\n",
      "\n",
      "# of elements: 67108864 = 131072 x 512\n",
      "populate double flatly takes: 0.0674115 sec\n",
      "populate double along last axis takes: 0.059984 sec\n",
      "populate double along first axis takes: 0.761658 sec\n",
      "ratio: 12.6977\n",
      "\n",
      "# of elements: 67108864 = 65536 x 1024\n",
      "populate double flatly takes: 0.0597462 sec\n",
      "populate double along last axis takes: 0.0550694 sec\n",
      "populate double along first axis takes: 0.806646 sec\n",
      "ratio: 14.6478\n",
      "\n",
      "# of elements: 67108864 = 32768 x 2048\n",
      "populate double flatly takes: 0.0606086 sec\n",
      "populate double along last axis takes: 0.053278 sec\n",
      "populate double along first axis takes: 0.572209 sec\n",
      "ratio: 10.7401\n",
      "\n",
      "# of elements: 67108864 = 16384 x 4096\n",
      "populate double flatly takes: 0.0643421 sec\n",
      "populate double along last axis takes: 0.0574708 sec\n",
      "populate double along first axis takes: 0.544686 sec\n",
      "ratio: 9.47762\n",
      "\n",
      "# of elements: 67108864 = 8192 x 8192\n",
      "populate double flatly takes: 0.0608146 sec\n",
      "populate double along last axis takes: 0.0536951 sec\n",
      "populate double along first axis takes: 0.578041 sec\n",
      "ratio: 10.7652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show how striding affects population runtime.\n",
    "!make 02_locality ; ./02_locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Array majoring in numpy\n",
    "\n",
    "Array majoring is directly related to locality.  The difference in the performance of matrix-vector multiplication is show for row- and column-majoring arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 388 ms, total: 1.56 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dim = 10000\n",
    "float_rmajor = np.arange(dim*dim, dtype='float64').reshape((dim,dim))\n",
    "float_cmajor = float_rmajor.T.copy().T\n",
    "vec = np.arange(dim, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.2 ms, sys: 1.26 ms, total: 65.5 ms\n",
      "Wall time: 64.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_float_rmajor = np.dot(float_rmajor, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 138 ms, sys: 1.47 ms, total: 139 ms\n",
      "Wall time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_float_cmajor = np.dot(float_cmajor, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integer matrix-vector multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 s, sys: 390 ms, total: 1.52 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dim = 10000\n",
    "int_rmajor = np.arange(dim*dim, dtype='int64').reshape((dim,dim))\n",
    "int_cmajor = int_rmajor.T.copy().T\n",
    "vec = np.arange(dim, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81.6 ms, sys: 1.09 ms, total: 82.7 ms\n",
      "Wall time: 81.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_int_rmajor = np.dot(int_rmajor, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 815 ms, sys: 2.01 ms, total: 817 ms\n",
      "Wall time: 816 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_int_cmajor = np.dot(int_cmajor, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance difference of integer arrays is much larger than floating-point arrays.  Note that `double` and `int64` both take 8 bytes.  Reason: LAPACK / MKL / vecLib.\n",
    "\n",
    "For the same reason, the floating-point multiplication is slightly faster than the integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling\n",
    "\n",
    "This is a naive implementation of matrix-matrix multiplication:\n",
    "\n",
    "```cpp\n",
    "for (size_t i=0; i<mat1.nrow(); ++i)\n",
    "{\n",
    "    for (size_t k=0; k<mat2.ncol(); ++k)\n",
    "    {\n",
    "        double v = 0;\n",
    "        for (size_t j=0; j<mat1.ncol(); ++j)\n",
    "        {\n",
    "            v += mat1(i,j) * mat2(j,k);\n",
    "        }\n",
    "        ret(i,k) = v;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The matrices are row-major.  The stride for the second matrix is `ncol2`, so it doesn't have good locality.  This naive implementation is clear, but the performance is bad.\n",
    "\n",
    "Matrix-matrix multiplication is one of the most important problems for numerical calculation, and there are many techniques available for making it fast.  Most if not all of them are about hiding the memory access latency.  Tiling is a basic technique that delivers impressive speed-up by reordering the calculation and making it cache-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 03_matrix_matrix 03_matrix_matrix.cpp\n",
      "Timing mkl: 0.0456862 second, 1.07374 Gflo, 23.5025 Gflops\n",
      "Timing indirect: 3.34553 second, 1.07374 Gflo, 0.320948 Gflops\n",
      "Timing direct: 3.30266 second, 1.07374 Gflo, 0.325114 Gflops\n",
      "Timing tiled 32: 1.17343 second, 1.07374 Gflo, 0.915044 Gflops\n",
      "Timing tiled 64: 0.445618 second, 1.07374 Gflo, 2.40956 Gflops\n",
      "Timing tiled 128: 0.756811 second, 1.07374 Gflo, 1.41877 Gflops\n",
      "Timing tiled 256: 0.835063 second, 1.07374 Gflo, 1.28582 Gflops\n",
      "Timing tiled 512: 0.817784 second, 1.07374 Gflo, 1.31299 Gflops\n",
      "Timing tiled 1024: 0.954288 second, 1.07374 Gflo, 1.12518 Gflops\n"
     ]
    }
   ],
   "source": [
    "# Show how tiling improves runtime performance.\n",
    "!make 03_matrix_matrix ; ./03_matrix_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Gflops: Giga FLoating Operations Per Seconds, Gflo: Giga FLoating Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Consult the data sheet of one x86 CPU and one Arm CPU.  Make a table for the line size of each of the cache levels, and compare the difference between the two CPUs.\n",
    "2. Write a program that uses tiling to speed up matrix-matrix multiplication, and do not require the matrix dimension to be multiples of the tile size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Computer Systems: A Programmer's Perspective, Chapter 6 The Memory Hierarchy, Randal E. Bryant and David R. O'Hallaron: https://csapp.cs.cmu.edu/\n",
    "* Gallery of Processor Cache Effects: http://igoro.com/archive/gallery-of-processor-cache-effects/\n",
    "* Lecture Notes of Applications of Parallel Computers by David Bindel: https://www.cs.cornell.edu/~bindel/class/cs5220-s10/slides/lec03.pdf\n",
    "* https://en.wikichip.org/wiki/WikiChip\n",
    "* https://www.uops.info/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
